---
title: "Capstone Project_1year default case"
author: "Loan Portfolio Prediction Team"
date: "2024-03-30"
output:
  word_document: default
  html_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE,  warning=FALSE}
library(readr)
library("tidyverse")
library("skimr")
library("readxl") # used to read excel files
library("dplyr") # used for data munging 
library("FNN") # used for knn regression (knn.reg function)
library("caret") # used for various predictive models
library("class") # for using confusion matrix function
library("rpart.plot") # used to plot decision tree
library("rpart")  # used for Regression tree
library("glmnet") # used for Lasso and Ridge regression
library('NeuralNetTools') # used to plot Neural Networks
library("PRROC") # top plot ROC curve
library("ROCR") # top plot lift curve
library("C50")
library("kableExtra")
library("formattable")
library("reticulate")
library(tm) #For text mining functionality
library(SnowballC) #For collapsing words to common roots
library(wordcloud) #For creating word cloud visualization
library(RColorBrewer) #For creating colorful graphs using pre-defined palettes
library(caTools) #For splitting the data  
library(ranger) # used to random forest
library(randomForest) # used to random forest
library(quanteda) #to get ngrams without using RWeka
library(corrplot)
library("FNN") # used for knn regression (knn.reg function)
library("class") # for using confusion matrix function
library("glmnet") # used for Lasso and Ridge regression
library('NeuralNetTools') # used to plot Neural Networks
library("tidyverse")
library("skimr")
library(randomForest)
library(readxl)
library(rpart)
library(DMwR)
library(ROSE)
library(DMwR)
library(pROC)
library(ROSE)  # for dataset balancing
library(broom)
```


#cleaning the train data set
```{r}
# Load the necessary library
#library(readxl)

# Specify the file path and sheet name
file_path <- "C:/Users/MY00002/Documents/NYU/Course/Capstone/Data/data/202001_202207_moped_expected payoff.xlsx"
sheet_name <- "defaulted in 1 year"

# Read the specified sheet and let read_excel infer the column types
whole_data <- read_excel(path = file_path, sheet = sheet_name)

# Exclude the first column, item column
whole_data <- select(whole_data, -1)

# Convert each column to factor explicitly
whole_data[] <- lapply(whole_data, as.factor)

# Check the classes of each column to confirm
attribute_classes <- sapply(whole_data, class)
print(attribute_classes)

```

```{r}
# Count the number of NA values in each column

na_count <- sapply(whole_data, function(x) sum(is.na(x)))

# Print the count of NA values for each column
print(na_count)

# Apply the table function to each column of the data frame
table_list <- lapply(whole_data, table)

# Print the table of each column
for (i in seq_along(table_list)) {
  cat("Table for column:", names(table_list)[i], "\n")
  print(table_list[[i]])
  cat("\n")
}


```


```{r}
library(dplyr)

whole_data <- whole_data %>%
  mutate(target_variable = ifelse(`defaulted year` == "0", 0, 1))

# Check the classes of each column to confirm
attribute_classes <- sapply(whole_data, class)
print(attribute_classes)
table(whole_data$target_variable)

```
```{r}
# Data pre-processing
# Select only the change variables and the response variable
predictor_columns  <- c(
  "G_total_limit",
  "G_risk_grade",
  "G_trade_bureau_cnt",
  "G_period_employment",
  "G_house_status",
  "G_net_income",
  #"G_race",
  "G_enquiry_cnt"
)

model_data <- whole_data %>%
  select(predictor_columns , target_variable ) %>%
  na.omit()

# Check the classes of each column to confirm
attribute_classes <- sapply(model_data, class)
print(attribute_classes)

table(model_data$target_variable)
```

```{r}
# Split the whole data into training and test sets (80% train, 20% test)

# Load caret library
library(caret)

# Check distribution of target_variable
table(model_data$target_variable)

# If distribution seems okay, manually split the data
set.seed(12345)

# Create a randomized index for splitting
index <- sample(seq_len(nrow(model_data)), size = 0.80 * nrow(model_data))

train_data <- model_data[index, ]
test_data <- model_data[-index, ]

table(train_data$target_variable)
table(test_data$target_variable)
```

#Task2.You will use the predictive modeling process we learnt in the class to land on a final binary classification model to use. 
#1.1 Buing a glm model with max payoff
```{r}
# Load necessary libraries
library(caret)
library(pROC)

# Define a function to print model performance statistics
print_model_statistics <- function(confusion_matrix) {
  overall_stats <- confusion_matrix[["overall"]]
  classwise_stats <- confusion_matrix[["byClass"]]
  
  cat("Accuracy :", round(overall_stats[["Accuracy"]], 3), "\n")
  cat("Precision:", round(classwise_stats[["Precision"]], 3), "\n")
  cat("Recall   :", round(classwise_stats[["Recall"]], 3), "\n")
  
  # Calculate F1 score
  precision <- classwise_stats[["Precision"]]
  recall <- classwise_stats[["Recall"]]
  f1_score <- 2 * (precision * recall) / (precision + recall)
  cat("F1 Score :", round(f1_score, 3), "\n")
}

# Train the logistic regression model
glm_1 <- glm(target_variable ~ ., data = train_data, family = "binomial")
summary(glm_1)
# Make probability predictions on the test set
glm_1_prob_predictions <- predict(glm_1, newdata = test_data, type = "response")

# Payoff matrix values
TN_value <- 28.03       # True Negative  (True non-defaulted)
FN_value <- -80.18      # False Negative (False non-defaulted)
FP_value <- 0           # False Positive (False defaulted)
TP_value <- 0           # True Positive  (True defaulted)

# Function to calculate the total payoff
calculate_total_payoff <- function(conf_matrix) {
  TN <- conf_matrix[1, 1]
  FN <- conf_matrix[1, 2]
  FP <- conf_matrix[2, 1]
  TP <- conf_matrix[2, 2]

  total_payoff <- TN * TN_value + FN * FN_value + FP * FP_value + TP * TP_value
  return(total_payoff)
}

# Calculating confusion matrices and payoffs for various thresholds
thresholds <- seq(0, 1, by = 0.05)
payoffs <- numeric(length(thresholds))

for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  predicted_classes <- ifelse(glm_1_prob_predictions >= threshold, 1, 0)
  conf_matrix <- table(Predicted = factor(predicted_classes, levels = c(0, 1)), 
                       Actual = test_data$target_variable)
  payoffs[i] <- calculate_total_payoff(conf_matrix)
  cat("Threshold:", threshold, "Payoff:", payoffs[i], "\n")
}

# Identifying the optimal threshold and payoff for glm_1
optimal_threshold_glm_1 <- thresholds[which.max(payoffs)]
optimal_payoff_glm_1 <- max(payoffs)

cat("Optimal Threshold for glm_1:", optimal_threshold_glm_1, "\n")
cat("Optimal Payoff for glm_1:", optimal_payoff_glm_1, "\n")

# Evaluate the model at the optimal threshold
glm_1_optimal_predictions <- ifelse(glm_1_prob_predictions >= optimal_threshold_glm_1, 1, 0)
glm_1_optimal_predictions_factor <- factor(glm_1_optimal_predictions, levels = c(0, 1))
test_data_target_factor <- factor(test_data$target_variable, levels = c(0, 1))

# Evaluate the model and print the confusion matrix
glm_1_confusion_matrix <- confusionMatrix(glm_1_optimal_predictions_factor, test_data_target_factor)
print(glm_1_confusion_matrix)

# Print model statistics for the optimal threshold
cat("Model Statistics for glm_1 with Optimal Threshold =", optimal_threshold_glm_1, ":\n")
print_model_statistics(glm_1_confusion_matrix)

# ROC and AUC
roc_obj_1 <- roc(as.numeric(as.character(test_data$target_variable)) - 1, glm_1_prob_predictions)
cat("AUC for glm_1:", auc(roc_obj_1), "\n")

# Plot the ROC curve
plot(roc_obj_1, main=paste("ROC Curve for glm_1 (Optimal Threshold =", optimal_threshold_glm_1, ")"))

```

# 1.2 glm_2, Logistic Regression Model buildind
# Before training, the dataset is resampled using the ovun.sample function from the ROSE package, creating a balanced training set.

```{r}
# Load necessary libraries
library(caret)
library(pROC)
library(ROSE)

# Prepare the training data
train_data$target_variable <- as.factor(train_data$target_variable)

# Balance the dataset using ovun.sample
set.seed(1234)  # for reproducibility
balanced_data <- ovun.sample(target_variable ~ ., data = train_data, method = "both", N = 2000)$data

# Train the logistic regression model on the balanced dataset
glm_2 <- glm(target_variable ~ ., data = balanced_data, family = "binomial")

# Make probability predictions on the test set
glm_2_prob_predictions <- predict(glm_2, newdata = test_data, type = "response")

# Payoff matrix values
TN_value <- 28.03
FN_value <- -80.18
FP_value <- 0
TP_value <- 0

# Function to calculate the total payoff
calculate_total_payoff <- function(conf_matrix) {
  TN <- conf_matrix[1, 1]
  FN <- conf_matrix[1, 2]
  FP <- conf_matrix[2, 1]
  TP <- conf_matrix[2, 2]

  total_payoff <- TN * TN_value + FN * FN_value + FP * FP_value + TP * TP_value
  return(total_payoff)
}

# Calculating confusion matrices and payoffs for various thresholds
thresholds <- seq(0, 1, by = 0.05)
payoffs <- numeric(length(thresholds))

for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  predicted_classes <- ifelse(glm_2_prob_predictions >= threshold, 1, 0)
  conf_matrix <- table(Predicted = factor(predicted_classes, levels = c(0, 1)), 
                       Actual = test_data$target_variable)
  payoffs[i] <- calculate_total_payoff(conf_matrix)
  cat("Threshold:", threshold, "Payoff:", payoffs[i], "\n")
}

# Identifying the optimal threshold and payoff for glm_2
optimal_threshold_index <- which.max(payoffs)
optimal_threshold_glm_2 <- thresholds[optimal_threshold_index]
optimal_payoff_glm_2 <- payoffs[optimal_threshold_index]

cat("Optimal Threshold for glm_2:", optimal_threshold_glm_2, "\n")
cat("Optimal Payoff for glm_2:", optimal_payoff_glm_2, "\n")

# Evaluate the model at the optimal threshold
glm_2_optimal_predictions <- ifelse(glm_2_prob_predictions >= optimal_threshold_glm_2, 1, 0)
glm_2_optimal_predictions_factor <- factor(glm_2_optimal_predictions, levels = c(0, 1))
test_data_target_factor <- factor(test_data$target_variable, levels = c(0, 1))

# Evaluate the model and print the confusion matrix
glm_2_confusion_matrix <- confusionMatrix(glm_2_optimal_predictions_factor, test_data_target_factor)
print(glm_2_confusion_matrix)

# Print model statistics for the optimal threshold
cat("Model Statistics for glm_2 with Optimal Threshold =", optimal_threshold_glm_2, ":\n")
print_model_statistics(glm_2_confusion_matrix)

# ROC and AUC
roc_obj_2 <- roc(as.numeric(as.character(test_data$target_variable)) - 1, glm_2_prob_predictions)
cat("AUC for glm_2:", auc(roc_obj_2), "\n")

# Plot the ROC curve
plot(roc_obj_2, main=paste("ROC Curve for glm_2 (Optimal Threshold =", optimal_threshold_glm_2, ")"))

```

## 1.3 glm_3, improved Logistic Regression Model buildind, "Uses SMOTE for balancing

```{r}
# Load necessary libraries
library(caret)
library(pROC)
library(DMwR)  # for SMOTE

# Define a function to print model performance statistics
print_model_statistics <- function(confusion_matrix) {
  overall_stats <- confusion_matrix[["overall"]]
  classwise_stats <- confusion_matrix[["byClass"]]
  
  cat("Accuracy :", round(overall_stats[["Accuracy"]], 3), "\n")
  cat("Precision:", round(classwise_stats[["Precision"]], 3), "\n")
  cat("Recall   :", round(classwise_stats[["Recall"]], 3), "\n")
  
  # Calculate F1 score
  precision <- classwise_stats[["Precision"]]
  recall <- classwise_stats[["Recall"]]
  f1_score <- 2 * (precision * recall) / (precision + recall)
  cat("F1 Score :", round(f1_score, 3), "\n")
}

# Convert the training data to a data frame for SMOTE
train_data_df <- as.data.frame(train_data)

# Apply SMOTE for balancing the dataset
set.seed(12345)  # For reproducibility of results
balanced_data_smote <- SMOTE(target_variable ~ ., train_data_df, perc.over = 100, k = 5, perc.under = 200)

# Train the logistic regression model on the balanced data
glm_3 <- glm(target_variable ~ ., data = balanced_data_smote, family = "binomial")

# Make probability predictions on the test set
glm_3_prob_predictions <- predict(glm_3, newdata = test_data, type = "response")

# Payoff matrix values
TN_value <- 28.03
FN_value <- -80.18
FP_value <- 0
TP_value <- 0

# Function to calculate the total payoff
calculate_total_payoff <- function(conf_matrix) {
  TN <- conf_matrix[1, 1]
  FN <- conf_matrix[1, 2]
  FP <- conf_matrix[2, 1]
  TP <- conf_matrix[2, 2]

  total_payoff <- TN * TN_value + FN * FN_value + FP * FP_value + TP * TP_value
  return(total_payoff)
}

# Calculating confusion matrices and payoffs for various thresholds
thresholds <- seq(0, 1, by = 0.05)
payoffs <- numeric(length(thresholds))

for (i in seq_along(thresholds)) {
  threshold <- thresholds[i]
  predicted_classes <- ifelse(glm_3_prob_predictions >= threshold, 1, 0)
  conf_matrix <- table(Predicted = factor(predicted_classes, levels = c(0, 1)), 
                       Actual = test_data$target_variable)
  payoffs[i] <- calculate_total_payoff(conf_matrix)
  
  # Print threshold and corresponding payoff
  cat("Threshold:", threshold, "Payoff:", payoffs[i], "\n")
}

# Identifying the optimal threshold and payoff for glm_3
optimal_threshold_index <- which.max(payoffs)
optimal_threshold_glm_3 <- thresholds[optimal_threshold_index]
optimal_payoff_glm_3 <- payoffs[optimal_threshold_index]

cat("Optimal Threshold for glm_3:", optimal_threshold_glm_3, "\n")
cat("Optimal Payoff for glm_3:", optimal_payoff_glm_3, "\n")

# Evaluate the model at the optimal threshold
glm_3_optimal_predictions <- ifelse(glm_3_prob_predictions >= optimal_threshold_glm_3, 1, 0)
glm_3_optimal_predictions_factor <- factor(glm_3_optimal_predictions, levels = c(0, 1))
test_data_target_factor <- factor(test_data$target_variable, levels = c(0, 1))

# Evaluate the model and print the confusion matrix
glm_3_confusion_matrix <- confusionMatrix(glm_3_optimal_predictions_factor, test_data_target_factor)
print(glm_3_confusion_matrix)

# Print model statistics for the optimal threshold
cat("Model Statistics for glm_3 with Optimal Threshold =", optimal_threshold_glm_3, ":\n")
print_model_statistics(glm_3_confusion_matrix)

# ROC and AUC
roc_obj_3 <- roc(as.numeric(as.character(test_data$target_variable)) - 1, glm_3_prob_predictions)
cat("AUC for glm_3:", auc(roc_obj_3), "\n")

# Plot the ROC curve
plot(roc_obj_3, main=paste("ROC Curve for glm_3 (Optimal Threshold =", optimal_threshold_glm_3, ")"))

```


#Performace comparison each model
```{r}
# Extract statistics from confusion matrix and ROC object
get_model_stats <- function(conf_matrix, roc_obj) {
  overall_stats <- conf_matrix[["overall"]]
  class_stats <- conf_matrix[["byClass"]]
  auc_value <- auc(roc_obj)
  
  accuracy <- overall_stats["Accuracy"]
  precision <- class_stats["Precision"]
  recall <- class_stats["Sensitivity"]  # Sensitivity is the same as Recall
  specificity <- class_stats["Specificity"]
  f1_score <- 2 * (precision * recall) / (precision + recall)
  sum_sens_spec <- recall + specificity
  
  return(c(accuracy, precision, recall, f1_score, sum_sens_spec, auc_value))
}

# Get statistics for each model
glm_1_stats <- c("glm_1", "Threshold=" %>% paste0(optimal_threshold_glm_1), optimal_threshold_glm_1, optimal_payoff_glm_1, get_model_stats(glm_1_confusion_matrix, roc_obj_1))
glm_2_stats <- c("glm_2", "Resampled with ROSE, Threshold=" %>% paste0(optimal_threshold_glm_2), optimal_threshold_glm_2, optimal_payoff_glm_2, get_model_stats(glm_2_confusion_matrix, roc_obj_2))
glm_3_stats <- c("glm_3", "SMOTE, Threshold=" %>% paste0(optimal_threshold_glm_3), optimal_threshold_glm_3, optimal_payoff_glm_3, get_model_stats(glm_3_confusion_matrix, roc_obj_3))

# Combine into a matrix
model_metrics <- rbind(glm_1_stats, glm_2_stats, glm_3_stats)

colnames(model_metrics) <- c("Model", "Description", "Optimal Threshold", "Optimal Payoff", "Accuracy", "Precision", "Recall", "F1 Score", "Sum of Sensitivity and Specificity", "AUC")

print(model_metrics)
```
